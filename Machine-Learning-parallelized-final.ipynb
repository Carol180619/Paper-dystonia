{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71bcce10",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from numpy import matrix \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import joblib\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,accuracy_score,f1_score,recall_score,precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import joblib\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,accuracy_score,f1_score,recall_score,precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,  precision_score, recall_score, classification_report, confusion_matrix\n",
    "import sklearn.metrics\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ec108",
   "metadata": {},
   "source": [
    "# READ MATRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy= [pd.read_csv(n,sep=\",\", index_col=False,header=None) for n in glob.glob(r'PATH OF HEALTHY MATRICES/*csv')]\n",
    "dystonia= [pd.read_csv(n,sep=\",\", index_col=False,header=None) for n in glob.glob(r'PATH OF DYSTONIA MATRICES/*csv')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad8bc8",
   "metadata": {},
   "source": [
    "# FLATTEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ec722",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_replace_healthy=[]\n",
    "list_flatten_healthy=[]\n",
    "def flatten_matrix(x):\n",
    "    test1 = np.array(x)\n",
    "    flatten= matrix.flatten(test1)\n",
    "    return flatten\n",
    "\n",
    "for i in healthy:\n",
    "    list_healthy.append(np.nan_to_num(i))\n",
    "  \n",
    "     \n",
    "for i in list_replace_healthy:\n",
    "    list_flatten_healthy.append(flatten_matrix(i))\n",
    "\n",
    "print(len(list_replace_healthy))\n",
    "print(len(list_flatten_healthy))\n",
    "\n",
    "\n",
    "list_replace_disease=[]\n",
    "list_flatten_disease=[]\n",
    "for i in dystonia:\n",
    "    list_replace_disease.append(np.nan_to_num(i))\n",
    "     \n",
    "for i in list_replace_disease:\n",
    "    list_flatten_disease.append(flatten_matrix(i))\n",
    "print(len(list_replace_disease))\n",
    "print(len(list_flatten_disease))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0bb52",
   "metadata": {},
   "source": [
    "# LABELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_label = [1]*len(list_flatten_disease)\n",
    "no_label = [0]*len(list_flatten_healthy)\n",
    "\n",
    "labels = yes_label + no_label\n",
    "#\n",
    "print(len(labels))\n",
    "y=np.nan_to_num(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b075e",
   "metadata": {},
   "source": [
    "# SPLITING AND  STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myScaler = preprocessing.StandardScaler()\n",
    "X= np.concatenate([list_flatten_disease,list_flatten_healthy],axis=0)\n",
    "X = myScaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.25, shuffle=True, \n",
    "    random_state = 1234)\n",
    "X_train= np.nan_to_num(X_train)\n",
    "\n",
    "X_test= np.nan_to_num(X_test)\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec87567",
   "metadata": {},
   "source": [
    "# DEFINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfccb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def evaluate_model(model, param_grid, X, y):\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f = []\n",
    "    accuracy = []\n",
    "    roc = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        xtr, xvl = X.loc[train_index], X.loc[test_index]\n",
    "        ytr, yvl = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        ytr = ytr.to_numpy().reshape(len(ytr),)\n",
    "        clf = GridSearchCV(model, param_grid, scoring='roc_auc')\n",
    "        clf.fit(xtr, ytr)\n",
    "        clf_best = clf.best_estimator_\n",
    "\n",
    "        y_pred = clf.best_estimator_.predict(xvl)\n",
    "\n",
    "        yvl = yvl.to_numpy().reshape(len(yvl),)\n",
    "        n_classes = len(np.unique(yvl))\n",
    "        yvl1 = label_binarize(yvl, classes=np.arange(n_classes))\n",
    "        y_pred1 = label_binarize(y_pred, classes=np.arange(n_classes))\n",
    "\n",
    "        roc.append(roc_auc_score(yvl,y_pred))\n",
    "        accuracy.append(accuracy_score(yvl, y_pred))\n",
    "        recall.append(recall_score(yvl,y_pred))\n",
    "        precision.append(precision_score(yvl,y_pred,average='weighted'))\n",
    "      \n",
    "        f.append(sklearn.metrics.f1_score(yvl,y_pred, average='weighted'))\n",
    "\n",
    "    return (np.mean(roc), np.std(roc), np.mean(accuracy), np.std(accuracy),\n",
    "            np.mean(recall), np.std(recall), np.mean(precision), np.std(precision), clf_best)\n",
    "\n",
    "def parallel_evaluate(model_name, X_train, y_train):\n",
    "    if model_name == 'RF':\n",
    "        param_grid = {\n",
    "            'n_estimators': [1, 2, 3, 5, 10, 30, 50, 100, 200, 300, 500]\n",
    "        }\n",
    "        model = RandomForestClassifier(random_state=1234)\n",
    "    elif model_name == 'SVM':\n",
    "        param_grid = {'kernel': ['rbf', 'poly', 'sigmoid'], 'gamma': [0.1, 0.01, 1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}\n",
    "        model = SVC(probability=True, random_state=1234)\n",
    "    elif model_name == 'NB':\n",
    "        param_grid = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "        model = GaussianNB()\n",
    "    elif model_name == 'MLP':\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['adam', 'sgd'],\n",
    "            'learning_rate': ['constant', 'adaptive'],\n",
    "        }\n",
    "        model = MLPClassifier(random_state=1234)\n",
    "    elif model_name == 'KNN':\n",
    "        param_grid = {\n",
    "            'n_neighbors': (1, 10, 1),\n",
    "            'leaf_size': (20, 40, 1),\n",
    "            'p': (1, 2),\n",
    "            'weights': ('uniform', 'distance'),\n",
    "            'metric': ('minkowski', 'chebyshev')\n",
    "        }\n",
    "        model = KNeighborsClassifier(random_state=1234)\n",
    "    elif model_name == 'LR':\n",
    "        param_grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n",
    "        model = LogisticRegression(random_state=1234)\n",
    "    elif model_name == 'XGboost':\n",
    "        param_grid = {\n",
    "            'min_child_weight': [1, 5, 10],\n",
    "            'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'max_depth': [3, 4, 5]\n",
    "        }\n",
    "        model = XGBClassifier(random_state=1234)\n",
    "\n",
    "    roc_mean, std_roc, accuracy_mean, std_accuracy,  recall_mean, std_recall, precision_mean, std_precision, clf = evaluate_model(model, param_grid, X_train, y_train)\n",
    "\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'AUC': roc_mean,\n",
    "        'AUC std': std_roc,\n",
    "        'Accuracy': accuracy_mean,\n",
    "        'Accuracy std': std_accuracy,\n",
    "        'Recall': recall_mean,\n",
    "        'Recall std': std_recall,\n",
    "        'Precision': precision_mean,\n",
    "        'Precision std': std_precision,\n",
    "        'clf_best': clf\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3057c",
   "metadata": {},
   "source": [
    "# CALLING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefb4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    models = ['LR']\n",
    "    pool = multiprocessing.Pool(processes=len(models))\n",
    "    results = pool.starmap(parallel_evaluate, [(model, X_train, y_train) for model in models])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Print the results\n",
    "    print('Train set performance')\n",
    "    print(df_results)\n",
    "    \n",
    "    # Test set performance for the best model\n",
    "    for model,best_model in zip(df_results['Model'],df_results['clf_best']):\n",
    "   # best_model = df_results.loc[df_results['AUC'].idxmax()]['clf_best']\n",
    "       # y_test1 = y_test.to_numpy()\n",
    "      #  y_test1 = y_test1.reshape(len(y_test1),)\n",
    "        n_classes = len(np.unique(y_test))\n",
    "        y_test1= label_binarize(y_test, classes=np.arange(n_classes))\n",
    "       \n",
    "        print(model)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        y_pred1 = label_binarize(y_pred_test, classes=np.arange(n_classes))\n",
    "        print('Test set performance')\n",
    "        print('AUC test:',   roc_auc_score(y_test,y_pred_test))\n",
    "        print('Accuracy test:', accuracy_score(y_test,y_pred_test))\n",
    "        print('F1 score test:', f1_score(y_test,y_pred_test, average=\"macro\", pos_label=0))\n",
    "        print('Recall test:', recall_score(y_test,y_pred_test,average=\"macro\", pos_label=0))\n",
    "        print('Precision test:', precision_score(y_test,y_pred_test,average=\"macro\", pos_label=0))\n",
    "\n",
    "        print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78cc938",
   "metadata": {},
   "source": [
    "# CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix')\n",
    "list_names=['HC','ULD']\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "cm_perc = cm / cm_sum.astype(float) * 100\n",
    "annot = np.empty_like(cm).astype(str)\n",
    "nrows, ncols = cm.shape\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        c = cm[i, j]\n",
    "        p = cm_perc[i, j]\n",
    "        if i == j:\n",
    "            s = cm_sum[i]\n",
    "            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "        elif c == 0:\n",
    "            annot[i, j] = ''\n",
    "        else:\n",
    "            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "cm = pd.DataFrame(cm)\n",
    "   # print(cm)\n",
    "cm.index.name = 'Actual'\n",
    "cm.columns.name = 'Predicted'\n",
    "sns.heatmap(cm, annot=annot, fmt='',cmap='rocket_r',xticklabels=list_names, yticklabels=list_names)\n",
    "plt.savefig(\"NAME TO SAVE FILES.pdf\",dpi=3000, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d9d97",
   "metadata": {},
   "source": [
    "# ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Roc curve')\n",
    "n_classes =2\n",
    "\n",
    "t1=sum(x==0 for x in y_pred_test-y_test)/len(y_test)\n",
    "\n",
    "    ### MACRO\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(pd.get_dummies(y_test))[:, i], np.array(pd.get_dummies(y_pred_test))[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "lw=2\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='slategray', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['purple', 'lightseagreen'])\n",
    "for i, color, name in zip(range(n_classes), colors,list_names):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label=\"ROC curve of class \"+ name + \" (area = {1:0.2f})\".format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], \"k--\",color='#cb416b', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.annotate(' Random Guess',(.5,.48),color='#cb416b')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"NAME TO SAVE FILES.pdf\",dpi=3000, bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a0c8c",
   "metadata": {},
   "source": [
    "# LEARNING CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bc7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sizes = np.linspace(0.1, 1, 10)\n",
    "visualizer = LearningCurve(\n",
    "    best_model, cv=10, scoring='accuracy', train_sizes=sizes, n_jobs=4\n",
    ")\n",
    "\n",
    "visualizer.fit(X, y)\n",
    "# Fit the data to the visualizer\n",
    "visualizer.show(outpath=\"NAME TO SAVE FILES.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd02f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
