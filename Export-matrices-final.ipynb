{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4adbd672",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db1f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from npeet import entropy_estimators as ee\n",
    "from pyinform import transfer_entropy \n",
    "import neurokit2 as nk\n",
    "from scipy import stats\n",
    "import glob\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from npeet import entropy_estimators as ee\n",
    "from pyinform import transfer_entropy \n",
    "import neurokit2 as nk\n",
    "from scipy import stats\n",
    "from numpy import matrix \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import antropy as ant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0c58a",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eed696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dystonia= [pd.read_csv(n,sep=\"\\t\", index_col=False,header=None, skiprows=[0]) for n in glob.glob(r'PATH FOLDER/*')]\n",
    "healthy= [pd.read_csv(n,sep=\"\\t\", index_col=False,header=None, skiprows=[0]) for n in glob.glob(r'PATH FOLDER/*')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906b452",
   "metadata": {},
   "source": [
    "# DEFINE MINIMUM SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e57ae6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45192\n",
      "51917\n"
     ]
    }
   ],
   "source": [
    "list_dystonia=[]\n",
    "list_healthy=[]\n",
    "\n",
    "for i in dystonia:\n",
    "    list_dystonia.append(len(i))\n",
    "\n",
    "for i in healthy:\n",
    "    list_healthy.append(len(i))\n",
    "\n",
    "print(min(list_dystonia))\n",
    "print(min(list_healthy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b6e39",
   "metadata": {},
   "source": [
    "# DEFINE SOME FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c5b2c1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "estimator1 = EmpiricalCovariance()\n",
    "estimator2 = LedoitWolf()\n",
    "def flatten_matrix(x):\n",
    "    test1 = np.array(x)\n",
    "    flatten= matrix.flatten(test1)\n",
    "    return flatten\n",
    "\n",
    "def replace(a):\n",
    "    y= np.matrix(a)\n",
    "       \n",
    "    y1 = a.replace('nan', 0, regex=True)\n",
    "    y2 = y1.values\n",
    "\n",
    "    y3 = y2.astype(float)\n",
    "    y4= np.matrix(y3)\n",
    "    return y4\n",
    "\n",
    "def flatten_matrix(x):\n",
    "    test1 = np.array(x)\n",
    "    #y = test1.astype(np.float)\n",
    "    #y= np.fromstring( test1, dtype=np.float )\n",
    "    flatten= matrix.flatten(test1)\n",
    "    return flatten\n",
    "def threshold1(matrix):\n",
    "    list_min = []\n",
    "    list_max = []\n",
    "    matrix1 = np.array(matrix, dtype=np.float)\n",
    "    threshold, upper, lower = 0.5, 1, 0\n",
    "    matrix1[matrix1 <= threshold] = lower\n",
    "    matrix1[matrix1 > threshold] = upper\n",
    "    return matrix1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85c1e4",
   "metadata": {},
   "source": [
    "# EXPORT DYSTONIA MATRICES AUTOMATICALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59afaa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the window sizes and corresponding folder names\n",
    "window_sizes = [1506]  # Add more sizes as needed\n",
    "window_names = ['10s']  # Corresponding window folder names\n",
    "\n",
    "channels = 32 # add number of channel\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'matrices-final/dystonia/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Loop through different window sizes and names\n",
    "\n",
    "for window, window_name in zip(window_sizes, window_names):\n",
    "    window_dir = os.path.join(output_dir, f'window_{window_name}')\n",
    "    os.makedirs(window_dir, exist_ok=True)\n",
    "\n",
    "            # Loop through each patient's EEG data\n",
    "    z=0\n",
    "    while z< len(dystonia):\n",
    "        for t in dystonia:\n",
    "            t= t[0:min(list_dystonia)]\n",
    "            N = len(t)\n",
    "            # Calculate the number of intervals without overlap\n",
    "            num_intervals = N // window\n",
    "        \n",
    "            # Loop through each interval for this patient\n",
    "            for k in range(num_intervals):\n",
    "        \n",
    "                start = k * window\n",
    "                end = start + window\n",
    "\n",
    "                # Extract the interval data\n",
    "                interval_data = t.iloc[start:end, :].values\n",
    "                 \n",
    "                 \n",
    "                # Calculate Spearman correlation matrix for the interval data\n",
    "                spearman_matrix, _ = stats.spearmanr(interval_data)\n",
    "\n",
    "                # Calculate Pearson correlation matrix for the interval data\n",
    "                pearson_matrix = np.corrcoef(interval_data, rowvar=False)\n",
    "\n",
    "                # Calculate Canonical correlation matrix for the interval data\n",
    "                estimator1.fit(interval_data)\n",
    "                canonical_matrix = estimator1.covariance_\n",
    "\n",
    "                # Calculate LW correlation matrix for the interval data\n",
    "                estimator2.fit(interval_data)\n",
    "                lw_matrix = estimator2.covariance_\n",
    "\n",
    "                # Calculate TE correlation matrix for the interval data\n",
    "                interval_data_normalized = min_max_scaler.fit_transform(interval_data)\n",
    "                interval_data_threshold = threshold1(interval_data_normalized)\n",
    "\n",
    "                M = np.zeros((channels, channels))\n",
    "                for i in range(channels):\n",
    "                    P1 = interval_data_threshold[i]\n",
    "                    for j in range(i + 1, channels):\n",
    "                        P2 = interval_data_threshold[j]\n",
    "\n",
    "                        try:\n",
    "                            M[i][j] = M[j][i] = transfer_entropy(P1, P2, k=2)\n",
    "                        except ValueError:  # raised if `y` is empty.\n",
    "                            pass\n",
    "                    M[i][i] = 0.\n",
    "\n",
    "                # Calculate MI correlation matrix for the interval data\n",
    "                M1 = np.zeros((channels, channels))\n",
    "                for i in range(channels):\n",
    "                    P1 = interval_data_threshold[i]\n",
    "                    for j in range(i + 1, channels):\n",
    "                        P2 = interval_data_threshold[j]\n",
    "\n",
    "                        try:\n",
    "                            M1[i][j] = M1[j][i] = nk.mutual_information(P1, P2, method=\"varoquaux\")\n",
    "                        except ValueError:  # raised if `y` is empty.\n",
    "                            pass\n",
    "                    M1[i][i] = 0.5\n",
    "            \n",
    "                    # Save TE correlation matrix to CSV\n",
    "                te_dir = os.path.join(window_dir, 'TE')  # Directory for TE matrices\n",
    "                os.makedirs(te_dir, exist_ok=True)\n",
    "                te_df = pd.DataFrame(M)\n",
    "                te_df.to_csv(os.path.join(te_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                    # Save MI correlation matrix to CSV\n",
    "                mi_dir = os.path.join(window_dir, 'MI')  # Directory for MI matrices\n",
    "                os.makedirs(mi_dir, exist_ok=True)\n",
    "                mi_df = pd.DataFrame(M1)\n",
    "                mi_df.to_csv(os.path.join(mi_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                 #Save LW correlation matrix to CSV\n",
    "                lw_dir = os.path.join(window_dir, 'LW')\n",
    "                os.makedirs(lw_dir, exist_ok=True)\n",
    "                lw_df = pd.DataFrame(lw_matrix)\n",
    "                lw_df.to_csv(os.path.join(lw_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                # Save Canonical correlation matrix to CSV\n",
    "                canonical_dir = os.path.join(window_dir, 'canonical')\n",
    "                os.makedirs(canonical_dir, exist_ok=True)\n",
    "                canonical_df = pd.DataFrame(canonical_matrix)\n",
    "                canonical_df.to_csv(os.path.join(canonical_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                # Save Spearman correlation matrix to CSV\n",
    "                spearman_dir = os.path.join(window_dir, 'spearman')\n",
    "                os.makedirs(spearman_dir, exist_ok=True)\n",
    "                spearman_df = pd.DataFrame(spearman_matrix)\n",
    "                spearman_df.to_csv(os.path.join(spearman_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                # Save Pearson correlation matrix to CSV\n",
    "                pearson_dir = os.path.join(window_dir, 'pearson')\n",
    "                os.makedirs(pearson_dir, exist_ok=True)\n",
    "                pearson_df = pd.DataFrame(pearson_matrix)\n",
    "                pearson_df.to_csv(os.path.join(pearson_dir,f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)    \n",
    "                \n",
    "                z= z+1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bda08",
   "metadata": {},
   "source": [
    "# EXPORT HEALTHY MATRICES AUTOMATICALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = 'matrices-final/healthy/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Loop through different window sizes and names\n",
    "\n",
    "for window, window_name in zip(window_sizes, window_names):\n",
    "    window_dir = os.path.join(output_dir, f'window_{window_name}')\n",
    "    os.makedirs(window_dir, exist_ok=True)\n",
    "\n",
    "            # Loop through each patient's EEG data\n",
    "    z=0\n",
    "    while z< len(healthy):\n",
    "        for t in healthy:\n",
    "            t= t[0:min(list_healthy)]\n",
    "            N = len(t)\n",
    "            # Calculate the number of intervals without overlap\n",
    "            num_intervals = N // window\n",
    "        \n",
    "            # Loop through each interval for this patient\n",
    "            for k in range(num_intervals):\n",
    "        \n",
    "                start = k * window\n",
    "                end = start + window\n",
    "\n",
    "                # Extract the interval data\n",
    "                interval_data = t.iloc[start:end, :].values\n",
    "                 \n",
    "                 \n",
    "                # Calculate Spearman correlation matrix for the interval data\n",
    "                spearman_matrix, _ = stats.spearmanr(interval_data)\n",
    "\n",
    "                # Calculate Pearson correlation matrix for the interval data\n",
    "                pearson_matrix = np.corrcoef(interval_data, rowvar=False)\n",
    "\n",
    "                # Calculate Canonical correlation matrix for the interval data\n",
    "                estimator1.fit(interval_data)\n",
    "                canonical_matrix = estimator1.covariance_\n",
    "\n",
    "                # Calculate LW correlation matrix for the interval data\n",
    "                estimator2.fit(interval_data)\n",
    "                lw_matrix = estimator2.covariance_\n",
    "\n",
    "                # Calculate TE correlation matrix for the interval data\n",
    "                interval_data_normalized = min_max_scaler.fit_transform(interval_data)\n",
    "                interval_data_threshold = threshold1(interval_data_normalized)\n",
    "\n",
    "                M = np.zeros((channels, channels))\n",
    "                for i in range(channels):\n",
    "                    P1 = interval_data_threshold[i]\n",
    "                    for j in range(i + 1, channels):\n",
    "                        P2 = interval_data_threshold[j]\n",
    "\n",
    "                        try:\n",
    "                            M[i][j] = M[j][i] = transfer_entropy(P1, P2, k=2)\n",
    "                        except ValueError:  # raised if `y` is empty.\n",
    "                            pass\n",
    "                    M[i][i] = 0.\n",
    "\n",
    "                # Calculate MI correlation matrix for the interval data\n",
    "                M1 = np.zeros((channels, channels))\n",
    "                for i in range(channels):\n",
    "                    P1 = interval_data_threshold[i]\n",
    "                    for j in range(i + 1, channels):\n",
    "                        P2 = interval_data_threshold[j]\n",
    "\n",
    "                        try:\n",
    "                            M1[i][j] = M1[j][i] = nk.mutual_information(P1, P2, method=\"varoquaux\")\n",
    "                        except ValueError:  # raised if `y` is empty.\n",
    "                            pass\n",
    "                    M1[i][i] = 0.5\n",
    "            \n",
    "                    # Save TE correlation matrix to CSV\n",
    "                te_dir = os.path.join(window_dir, 'TE')  # Directory for TE matrices\n",
    "                os.makedirs(te_dir, exist_ok=True)\n",
    "                te_df = pd.DataFrame(M)\n",
    "                te_df.to_csv(os.path.join(te_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                    # Save MI correlation matrix to CSV\n",
    "                mi_dir = os.path.join(window_dir, 'MI')  # Directory for MI matrices\n",
    "                os.makedirs(mi_dir, exist_ok=True)\n",
    "                mi_df = pd.DataFrame(M1)\n",
    "                mi_df.to_csv(os.path.join(mi_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                 #Save LW correlation matrix to CSV\n",
    "                lw_dir = os.path.join(window_dir, 'LW')\n",
    "                os.makedirs(lw_dir, exist_ok=True)\n",
    "                lw_df = pd.DataFrame(lw_matrix)\n",
    "                lw_df.to_csv(os.path.join(lw_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                # Save Canonical correlation matrix to CSV\n",
    "                canonical_dir = os.path.join(window_dir, 'canonical')\n",
    "                os.makedirs(canonical_dir, exist_ok=True)\n",
    "                canonical_df = pd.DataFrame(canonical_matrix)\n",
    "                canonical_df.to_csv(os.path.join(canonical_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                # Save Spearman correlation matrix to CSV\n",
    "                spearman_dir = os.path.join(window_dir, 'spearman')\n",
    "                os.makedirs(spearman_dir, exist_ok=True)\n",
    "                spearman_df = pd.DataFrame(spearman_matrix)\n",
    "                spearman_df.to_csv(os.path.join(spearman_dir, f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)\n",
    "\n",
    "                # Save Pearson correlation matrix to CSV\n",
    "                pearson_dir = os.path.join(window_dir, 'pearson')\n",
    "                os.makedirs(pearson_dir, exist_ok=True)\n",
    "                pearson_df = pd.DataFrame(pearson_matrix)\n",
    "                pearson_df.to_csv(os.path.join(pearson_dir,f'patient_{str(z)}_interval_{str(k)}_te.csv'), sep=',', header=False, index=False)    \n",
    "                \n",
    "                z= z+1    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
